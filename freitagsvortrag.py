# -*- coding: utf-8 -*-
"""Freitagsvortrag.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ck3Az8LLLcSdY_81XWPjxZDYiUX7gwEs

Schritt 0: Bibliotheken importieren
"""

# Commented out IPython magic to ensure Python compatibility.
import random
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
from pylab import rcParams

rcParams['figure.figsize'] = 12, 6
sns.set(style='whitegrid', palette='muted', font_scale=1.5)

"""Schritt 1: Sequenzdaten generieren

Wir erzeugen Zeichenketten des folgenden Formats: ([4 Buchstaben A-F][1 Ziffer 0-2][3 Buchstaben QWOPZXML]), und generieren 25K Sequenzen dieses Formats.

"""

first_letters =  'ABCDEF'
second_numbers = '120'
last_letters = 'QWOPZXML'

def get_random_string():
    str1 = ''.join(random.choice(first_letters) for i in range(4))
    str2 = random.choice(second_numbers)
    str3 = ''.join(random.choice(last_letters) for i in range(3))
    return str1+str2+str3
    
print(get_random_string())

#Erzuegen von 25.000 Sequenzen
random_sequences = [get_random_string() for i in range(25000)]
random_sequences[1:5]

"""Schritt 2: Anomalien hinzufügen"""

# Hinzufügen von 5 Anomalien zu unserer Liste von Sequenzen
random_sequences.extend(['XYDC2DCA', 'TXSX1ABC','RNIU4XRE','AABDXUEI','SDRAC5RF'])
#Speichern in DataFrame
seqs_ds = pd.DataFrame(random_sequences)

"""Schritt 3: Encoding - Kodierung unserer Sequenzen in Zahlen"""

#Build the char index that we will use to encode seqs to numbers
char_index = '0abcdefghijklmnopqrstuvwxyz'
char_index +='ABCDEFGHIJKLMNOPQRSTUVWXYZ'
char_index += '123456789'
char_index += '().,-/+=&$?@#!*:;_[]|%⸏{}\"\'' + ' ' +'\\'

char_to_int = dict((c, i) for i, c in enumerate(char_index))
int_to_char = dict((i, c) for i, c in enumerate(char_index))

from keras.preprocessing.sequence import pad_sequences

#Funktion, die alphanummerische Sequenzen in Integer-Array kodiert 
def encode_sequence_list(seqs, feat_n=0):
    encoded_seqs = []
    for seq in seqs:
        encoded_seq = [char_to_int[c] for c in seq]
        encoded_seqs.append(encoded_seq)
    if(feat_n > 0):
        encoded_seqs.append(np.zeros(feat_n))
    return pad_sequences(encoded_seqs, padding='post')

# Encoding
encoded_seqs = encode_sequence_list(random_sequences)
# Liste von Sequnezen vermischen
np.random.shuffle(encoded_seqs)

encoded_seqs.shape

print(random_sequences[10])
print(encoded_seqs[10])

"""Schritt 4: Data Scaling und Data Set Split"""

from sklearn.preprocessing import StandardScaler, MinMaxScaler
scaler = MinMaxScaler()
scaled_seqs = scaler.fit_transform(encoded_seqs)
X_train = scaled_seqs[:20000]
X_test = scaled_seqs[20000:]

"""Schritt 5: Autoencoder bauen"""

#Zusätzliche Bibliotheken
from keras.models import Model, load_model
from keras.layers import Input, Dense, Dropout, LSTM
from keras.callbacks import ModelCheckpoint, TensorBoard
from keras import regularizers
from keras.utils.vis_utils import plot_model

#Festlegen der Dimensionen
input_dim = X_train.shape[1] #features
print(input_dim)
encoding_dim = 8
hidden_dim = int(encoding_dim / 2)

#Trainingsparameter
nb_epoch = 30
batch_size = 128
learning_rate = 0.01

#Input Layer
input_layer = Input(shape=(input_dim, ))

#Encoding Layer
encoder = Dense(encoding_dim, activation="tanh", activity_regularizer=regularizers.l1(10e-5))(input_layer)
encoder = Dense(hidden_dim, activation="relu")(encoder)

#Decoding Layer
decoder = Dense(hidden_dim, activation='relu')(encoder)
decoder = Dense(encoding_dim, activation='relu')(encoder)
decoder = Dense(input_dim, activation='tanh')(decoder)

autoencoder = Model(inputs=input_layer, outputs=decoder)
autoencoder.summary()
plot_model(autoencoder, show_shapes=True, show_layer_names=True)

"""Schritt 6: Modell trainieren"""

autoencoder.compile(optimizer='adam', 
                    loss='mean_squared_error', 
                    metrics=['accuracy'])

checkpointer = ModelCheckpoint(filepath="model_seqs2.h5",
                               verbose=0,
                               save_best_only=True)

tensorboard = TensorBoard(log_dir='./logs',
                          histogram_freq=0,
                          write_graph=True,
                          write_images=True)

history = autoencoder.fit(X_train, X_train,
                    epochs=nb_epoch,
                    batch_size=batch_size,
                    shuffle=True,
                    validation_data=(X_test, X_test),
                    verbose=1,
                    callbacks=[checkpointer, tensorboard]).history

"""Schritt 7: Rekonstruktionsfehler bestimmen"""

#Model Loss
plt.plot(history['loss'])
plt.plot(history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right');

#MSE berechnen
predictions = autoencoder.predict(scaled_seqs)
mse = np.mean(np.power(scaled_seqs - predictions, 2), axis=1) #Distanz zwischen originalem Datenpunkt und rekonstruierteDatenpunkt
print('MSE:', np.quantile(mse, 0.9998)) # =>  0,9998% quantil - nur 0.0002% haben höhere Fehlerrate

#encode all the data
encoded_seqs = encode_sequence_list(seqs_ds.iloc[:,0])
#scale it
scaled_data = MinMaxScaler().fit_transform(encoded_seqs)
#predict it
predicted = autoencoder.predict(scaled_data)
#get the error term
mse = np.mean(np.power(scaled_data - predicted, 2), axis=1)
#now add them to our data frame
seqs_ds['MSE'] = mse

display(seqs_ds)

"""Schritt 8: Schwellwert bestimmen und Anomalien kennzeichnen"""

mse_threshold = np.quantile(seqs_ds['MSE'], 0.9998)
print(f'MSE 0.9998 threshhold:{mse_threshold}')

seqs_ds['MSE_Outlier'] = 0
seqs_ds.loc[seqs_ds['MSE'] > mse_threshold, 'MSE_Outlier'] = 1

print(f"Num of MSE outlier:{seqs_ds['MSE_Outlier'].sum()}")

#seqs_ds.iloc[25000:]
seqs_ds.loc[seqs_ds['MSE_Outlier']==1]

['XYDC2DCA', 'TXSX1ABC','RNIU4XRE','AABDXUEI','SDRAC5RF']